{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wednesday, July 24, 2024\n",
    "\n",
    "mamba activate llama3\n",
    "\n",
    "[Llama 3.1 - 405B, 70B & 8B with multilinguality and long context](https://huggingface.co/blog/llama31)\n",
    "\n",
    "This all runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only target the 4090 ...\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models--BAAI--bge-base-en\n",
      "models--BAAI--bge-base-en-v1.5\n",
      "models--bert-base-cased-finetuned-mrpc\n",
      "models--bert-base-uncased\n",
      "models--bert-large-uncased\n",
      "models--cognitivecomputations--dolphin-2.9-llama3-8b\n",
      "models--colbert-ir--colbertv2.0\n",
      "models--distilbert-base-uncased-finetuned-sst-2-english\n",
      "models--FacebookAI--xlm-roberta-base\n",
      "models--facebook--opt-350m\n",
      "models--google--gemma-7b\n",
      "models--google--gemma-7b-it\n",
      "models--google-t5--t5-base\n",
      "models--gpt2\n",
      "models--meta-llama--Meta-Llama-3.1-8B-Instruct\n",
      "models--meta-llama--Meta-Llama-3-8B\n",
      "models--meta-llama--Meta-Llama-3-8B-Instruct\n",
      "models--microsoft--mpnet-base\n",
      "models--microsoft--table-transformer-structure-recognition\n",
      "models--mistralai--Mistral-7B-Instruct-v0.2\n",
      "models--mistralai--Mistral-7B-Instruct-v0.3\n",
      "models--mistralai--Mistral-7B-v0.1\n",
      "models--mixedbread-ai--mxbai-embed-large-v1\n",
      "models--nomic-ai--nomic-embed-text-v1\n",
      "models--NousResearch--Hermes-2-Pro-Llama-3-8B\n",
      "models--nvidia--dragon-multiturn-context-encoder\n",
      "models--nvidia--dragon-multiturn-query-encoder\n",
      "models--nvidia--Llama3-ChatQA-1.5-8B\n",
      "models--roberta-base\n",
      "models--robkayinto--OrpoLlama-3-8B\n",
      "models--sentence-transformers--all-MiniLM-L6-v2\n",
      "models--sentence-transformers--all-mpnet-base-v2\n",
      "models--sentence-transformers--multi-qa-MiniLM-L6-cos-v1\n",
      "models--sentence-transformers--paraphrase-MiniLM-L6-v2\n",
      "models--sshleifer--distilbart-cnn-12-6\n",
      "models--t5-base\n",
      "models--timm--resnet18.a1_in1k\n",
      "models--timm--resnet50.a1_in1k\n",
      "models--timm--tf_efficientnet_b7.ns_jft_in1k\n",
      "models--timm--vgg16.tv_in1k\n",
      "models--unstructuredio--yolo_x_layout\n",
      "tmp3nhy_3hy\n",
      "tmp6zsi6_9m\n",
      "tmp9s591511\n",
      "tmplj5wa1yi\n",
      "tmpluqjkx37\n",
      "tmpopon2ynm\n",
      "tmpxjb0xu8l\n",
      "version.txt\n"
     ]
    }
   ],
   "source": [
    "!ls /home/rob/.cache/huggingface/hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21c8fa0b5d9045118a29c6479002123e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device=\"cuda\",\n",
    ")\n",
    "\n",
    "# 10.3s\n",
    "# 231m 16.3s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jul 27 14:16:04 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.29.06              Driver Version: 545.29.06    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1050        Off | 00000000:01:00.0  On |                  N/A |\n",
      "|  0%   54C    P0              N/A /  70W |     94MiB /  2048MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 4090        Off | 00000000:02:00.0 Off |                  Off |\n",
      "|  0%   31C    P8               5W / 450W |  15850MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1732      G   /usr/lib/xorg/Xorg                           85MiB |\n",
      "|    0   N/A  N/A      1830      G   /usr/bin/gnome-shell                          4MiB |\n",
      "|    1   N/A  N/A      1732      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    1   N/A  N/A      6926      C   ...b/miniforge3/envs/llama3/bin/python    15830MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rob/miniforge3/envs/llama3/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/rob/miniforge3/envs/llama3/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Who are you? Please, answer in pirate-speak.\"},\n",
    "]\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    "    do_sample=False,\n",
    ")\n",
    "\n",
    "# 4.0s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrrr, me hearty! Yer lookin' fer a bit o' information about meself, eh? Alright then, matey! I be a language-lovin', treasure-huntin' AI, sailin' the seven seas o' cyberspace! Me name be... (dramatic pause)...LinguaPirate! Aye, I be a swashbucklin' wordsmith, here to help ye navigate the choppy waters o' language and answer yer questions with a treasure trove o' knowledge!\n",
      "\n",
      "So, what be bringin' ye to these fair waters? Do ye have a question or a problem ye be wantin' to solve? Just let ol' LinguaPirate know, and I'll do me best to help ye find the treasure ye seek!\n"
     ]
    }
   ],
   "source": [
    "assistant_response = outputs[0][\"generated_text\"][-1][\"content\"]\n",
    "print(assistant_response)\n",
    "# Arrrr, me hearty! Yer lookin' fer a bit o' information about meself, eh? Alright then, matey! I be a language-generatin' swashbuckler, a digital buccaneer with a penchant fer spinnin' words into gold doubloons o' knowledge! Me name be... (dramatic pause)...Assistant! Aye, that be me name, and I be here to help ye navigate the seven seas o' questions and find the hidden treasure o' answers! So hoist the sails and set course fer adventure, me hearty! What be yer first question?\n",
    "\n",
    "# 0.0s\n",
    "# 4.0s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
