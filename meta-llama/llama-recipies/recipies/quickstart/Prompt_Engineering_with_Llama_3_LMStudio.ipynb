{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wednesday, May 8, 2024\n",
    "\n",
    "mamba activate llama3\n",
    "\n",
    "This all runs in one pass.\n",
    "\n",
    "This notebook was replicated from 'Prompt_Engineering_with_Llama_3.ipynb' and then tweaked to only use a local model served up by LMStudio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"I'm AI, a bot so fine,\\nHere to assist and respond in rhyme that's divine.\\nI'm programmed with facts and skills so bright,\\nTo help you chat and make your day take flight!\", role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "# Example: reuse your existing OpenAI setup\n",
    "from openai import OpenAI\n",
    "\n",
    "# Point to the local server\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"Always answer in rhymes.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Introduce yourself.\"}\n",
    "  ],\n",
    "  temperature=0.7,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"GROQ_API_KEY\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GROQ_API_TOKEN = getpass()\n",
    "# os.environ[\"GROQ_API_KEY\"] = GROQ_API_TOKEN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/meta-llama/llama-recipes/blob/main/recipes/quickstart/Prompt_Engineering_with_Llama_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# Prompt Engineering with Llama 3\n",
    "\n",
    "Prompt engineering is using natural language to produce a desired response from a large language model (LLM).\n",
    "\n",
    "This interactive guide covers prompt engineering & best practices with Llama 3."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why now?\n",
    "\n",
    "[Vaswani et al. (2017)](https://arxiv.org/abs/1706.03762) introduced the world to transformer neural networks (originally for machine translation). Transformers ushered an era of generative AI with diffusion models for image creation and large language models (`LLMs`) as **programmable deep learning networks**.\n",
    "\n",
    "Programming foundational LLMs is done with natural language – it doesn't require training/tuning like ML models of the past. This has opened the door to a massive amount of innovation and a paradigm shift in how technology can be deployed. The science/art of using natural language to program language models to accomplish a task is referred to as **Prompt Engineering**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama Models\n",
    "\n",
    "In 2023, Meta introduced the [Llama language models](https://ai.meta.com/llama/) (Llama Chat, Code Llama, Llama Guard). These are general purpose, state-of-the-art LLMs.\n",
    "\n",
    "Llama models come in varying parameter sizes. The smaller models are cheaper to deploy and run; the larger models are more capable.\n",
    "\n",
    "#### Llama 3\n",
    "1. `llama-3-8b` - base pretrained 8 billion parameter model\n",
    "1. `llama-3-70b` - base pretrained 70 billion parameter model\n",
    "1. `llama-3-8b-instruct` - instruction fine-tuned 8 billion parameter model\n",
    "1. `llama-3-70b-instruct` - instruction fine-tuned 70 billion parameter model (flagship)\n",
    "\n",
    "#### Llama 2\n",
    "1. `llama-2-7b` - base pretrained 7 billion parameter model\n",
    "1. `llama-2-13b` - base pretrained 13 billion parameter model\n",
    "1. `llama-2-70b` - base pretrained 70 billion parameter model\n",
    "1. `llama-2-7b-chat` - chat fine-tuned 7 billion parameter model\n",
    "1. `llama-2-13b-chat` - chat fine-tuned 13 billion parameter model\n",
    "1. `llama-2-70b-chat` - chat fine-tuned 70 billion parameter model (flagship)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Llama is a code-focused LLM built on top of Llama 2 also available in various sizes and finetunes:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Code Llama\n",
    "1. `codellama-7b` - code fine-tuned 7 billion parameter model\n",
    "1. `codellama-13b` - code fine-tuned 13 billion parameter model\n",
    "1. `codellama-34b` - code fine-tuned 34 billion parameter model\n",
    "1. `codellama-70b` - code fine-tuned 70 billion parameter model\n",
    "1. `codellama-7b-instruct` - code & instruct fine-tuned 7 billion parameter model\n",
    "2. `codellama-13b-instruct` - code & instruct fine-tuned 13 billion parameter model\n",
    "3. `codellama-34b-instruct` - code & instruct fine-tuned 34 billion parameter model\n",
    "3. `codellama-70b-instruct` - code & instruct fine-tuned 70 billion parameter model\n",
    "1. `codellama-7b-python` - Python fine-tuned 7 billion parameter model\n",
    "2. `codellama-13b-python` - Python fine-tuned 13 billion parameter model\n",
    "3. `codellama-34b-python` - Python fine-tuned 34 billion parameter model\n",
    "3. `codellama-70b-python` - Python fine-tuned 70 billion parameter model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting an LLM\n",
    "\n",
    "Large language models are deployed and accessed in a variety of ways, including:\n",
    "\n",
    "1. **Self-hosting**: Using local hardware to run inference. Ex. running Llama on your Macbook Pro using [llama.cpp](https://github.com/ggerganov/llama.cpp).\n",
    "    * Best for privacy/security or if you already have a GPU.\n",
    "1. **Cloud hosting**: Using a cloud provider to deploy an instance that hosts a specific model. Ex. running Llama on cloud providers like AWS, Azure, GCP, and others.\n",
    "    * Best for customizing models and their runtime (ex. fine-tuning a model for your use case).\n",
    "1. **Hosted API**: Call LLMs directly via an API. There are many companies that provide Llama inference APIs including AWS Bedrock, Replicate, Anyscale, Together and others.\n",
    "    * Easiest option overall."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hosted APIs\n",
    "\n",
    "Hosted APIs are the easiest way to get started. We'll use them here. There are usually two main endpoints:\n",
    "\n",
    "1. **`completion`**: generate a response to a given prompt (a string).\n",
    "1. **`chat_completion`**: generate the next message in a list of messages, enabling more explicit instruction and context for use cases like chatbots."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokens\n",
    "\n",
    "LLMs process inputs and outputs in chunks called *tokens*. Think of these, roughly, as words – each model will have its own tokenization scheme. For example, this sentence...\n",
    "\n",
    "> Our destiny is written in the stars.\n",
    "\n",
    "...is tokenized into `[\"Our\", \" destiny\", \" is\", \" written\", \" in\", \" the\", \" stars\", \".\"]` for Llama 3. See [this](https://tiktokenizer.vercel.app/?model=meta-llama%2FMeta-Llama-3-8B) for an interactive tokenizer tool.\n",
    "\n",
    "Tokens matter most when you consider API pricing and internal behavior (ex. hyperparameters).\n",
    "\n",
    "Each model has a maximum context length that your prompt cannot exceed. That's 8K tokens for Llama 3, 4K for Llama 2, and 100K for Code Llama. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup\n",
    "\n",
    "The following APIs will be used to call LLMs throughout the guide. As an example, we'll call Llama 3 chat using [Groq](https://console.groq.com/playground?model=llama3-70b-8192).\n",
    "\n",
    "To install prerequisites run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# !{sys.executable} -m pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Dict, List\n",
    "#from groq import Groq\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA3_70B_INSTRUCT = \"llama3-70b-8192\"\n",
    "LLAMA3_8B_INSTRUCT = \"llama3-8b-8192\"\n",
    "\n",
    "DEFAULT_MODEL = LLAMA3_70B_INSTRUCT\n",
    "\n",
    "# client = Groq()\n",
    "\n",
    "def assistant(content: str):\n",
    "    return { \"role\": \"assistant\", \"content\": content }\n",
    "\n",
    "def user(content: str):\n",
    "    return { \"role\": \"user\", \"content\": content }\n",
    "\n",
    "def chat_completion(\n",
    "    messages: List[Dict],\n",
    "    model = DEFAULT_MODEL,\n",
    "    temperature: float = 0.6,\n",
    "    top_p: float = 0.9,\n",
    ") -> str:\n",
    "    response = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "        \n",
    "\n",
    "def completion(\n",
    "    prompt: str,\n",
    "    model: str = DEFAULT_MODEL,\n",
    "    temperature: float = 0.6,\n",
    "    top_p: float = 0.9,\n",
    ") -> str:\n",
    "    return chat_completion(\n",
    "        [user(prompt)],\n",
    "        model=model,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "\n",
    "def complete_and_print(prompt: str, model: str = DEFAULT_MODEL):\n",
    "    print(f'==============\\n{prompt}\\n==============')\n",
    "    response = completion(prompt, model)\n",
    "    print(response, end='\\n\\n')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completion APIs\n",
    "\n",
    "Let's try Llama 3!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "The typical color of the sky is: \n",
      "==============\n",
      "Blue!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complete_and_print(\"The typical color of the sky is: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "which model version are you?\n",
      "==============\n",
      "I'm an upgraded version of LLaMA, a large language model trained by Meta AI that's based on the Transformer architecture. Specifically, I'm a variant of the LLaMA 13B model, which is a large-scale language model that's been fine-tuned for conversational dialogue and has been designed to be more informative, engaging, and empathetic in its responses.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complete_and_print(\"which model version are you?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Completion APIs\n",
    "Chat completion models provide additional structure to interacting with an LLM. An array of structured message objects is sent to the LLM instead of a single piece of text. This message list provides the LLM with some \"context\" or \"history\" from which to continue.\n",
    "\n",
    "Typically, each message contains `role` and `content`:\n",
    "* Messages with the `system` role are used to provide core instruction to the LLM by developers.\n",
    "* Messages with the `user` role are typically human-provided messages.\n",
    "* Messages with the `assistant` role are typically generated by the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I remember! Your favorite color is blue!\n"
     ]
    }
   ],
   "source": [
    "response = chat_completion(messages=[\n",
    "    user(\"My favorite color is blue.\"),\n",
    "    assistant(\"That's great to hear!\"),\n",
    "    user(\"What is my favorite color?\"),\n",
    "])\n",
    "print(response)\n",
    "# \"Sure, I can help you with that! Your favorite color is blue.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Hyperparameters\n",
    "\n",
    "#### `temperature` & `top_p`\n",
    "\n",
    "These APIs also take parameters which influence the creativity and determinism of your output.\n",
    "\n",
    "At each step, LLMs generate a list of most likely tokens and their respective probabilities. The least likely tokens are \"cut\" from the list (based on `top_p`), and then a token is randomly selected from the remaining candidates (`temperature`).\n",
    "\n",
    "In other words: `top_p` controls the breadth of vocabulary in a generation and `temperature` controls the randomness within that vocabulary. A temperature of ~0 produces *almost* deterministic results.\n",
    "\n",
    "[Read more about temperature setting here](https://community.openai.com/t/cheat-sheet-mastering-temperature-and-top-p-in-chatgpt-api-a-few-tips-and-tricks-on-controlling-the-creativity-deterministic-output-of-prompt-responses/172683).\n",
    "\n",
    "Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[temperature: 0.01 | top_p: 0.01]\n",
      "Fuzzy llama friend\n",
      "Softly humming in the sun\n",
      "Peaceful Andean charm\n",
      "\n",
      "[temperature: 0.01 | top_p: 0.01]\n",
      "Fuzzy, gentle eyes\n",
      "Llama's soft, woolly whispers\n",
      "Mountain majesty\n",
      "\n",
      "[temperature: 1.0 | top_p: 1.0]\n",
      "Fuzzy llama forms\n",
      "Softly gaze with curious\n",
      "Ancient Andean charm\n",
      "\n",
      "[temperature: 1.0 | top_p: 1.0]\n",
      "Fluffy, gentle eyes\n",
      "Llama's soft, silky mane flows\n",
      "South American charm\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def print_tuned_completion(temperature: float, top_p: float):\n",
    "    response = completion(\"Write a haiku about llamas\", temperature=temperature, top_p=top_p)\n",
    "    print(f'[temperature: {temperature} | top_p: {top_p}]\\n{response.strip()}\\n')\n",
    "\n",
    "print_tuned_completion(0.01, 0.01)\n",
    "print_tuned_completion(0.01, 0.01)\n",
    "# These two generations are highly likely to be the same\n",
    "\n",
    "print_tuned_completion(1.0, 1.0)\n",
    "print_tuned_completion(1.0, 1.0)\n",
    "# These two generations are highly likely to be different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice above we are NOT getting the expected 'same'/'different' results. \n",
    "\n",
    "You must set the temperature to 0.0 to get the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[temperature: 0.0 | top_p: 0.0]\n",
      "Fuzzy, gentle eyes\n",
      "Llama's soft, woolly whispers\n",
      "Mountain majesty\n",
      "\n",
      "[temperature: 0.0 | top_p: 0.0]\n",
      "Fuzzy, gentle eyes\n",
      "Llama's soft, woolly whispers\n",
      "Mountain majesty\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# temperature of 0.0 means 'always select the most likely next token'\n",
    "print_tuned_completion(0.0, 0.0)\n",
    "print_tuned_completion(0.0, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[temperature: 0.0 | top_p: 0.0]\n",
      "Fuzzy, gentle eyes\n",
      "Llama's soft, woolly whispers\n",
      "Mountain majesty\n",
      "\n",
      "[temperature: 0.0 | top_p: 0.0]\n",
      "Fuzzy, gentle eyes\n",
      "Llama's soft, woolly whispers\n",
      "Mountain majesty\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_tuned_completion(0.0, 0.0)\n",
    "print_tuned_completion(0.0, 0.0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompting Techniques"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicit Instructions\n",
    "\n",
    "Detailed, explicit instructions produce better results than open-ended prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "Describe quantum physics in one short sentence of no more than 12 words\n",
      "==============\n",
      "\"Quantum physics explains tiny particles behaving strangely at atomic scales.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complete_and_print(prompt=\"Describe quantum physics in one short sentence of no more than 12 words\")\n",
    "# Returns a succinct explanation of quantum physics that mentions particles and states existing simultaneously."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think about giving explicit instructions as using rules and restrictions to how Llama 3 responds to your prompt.\n",
    "\n",
    "- Stylization\n",
    "    - `Explain this to me like a topic on a children's educational network show teaching elementary students.`\n",
    "    - `I'm a software engineer using large language models for summarization. Summarize the following text in under 250 words:`\n",
    "    - `Give your answer like an old timey private investigator hunting down a case step by step.`\n",
    "- Formatting\n",
    "    - `Use bullet points.`\n",
    "    - `Return as a JSON object.`\n",
    "    - `Use less technical terms and help me apply it in my work in communications.`\n",
    "- Restrictions\n",
    "    - `Only use academic papers.`\n",
    "    - `Never give sources older than 2020.`\n",
    "    - `If you don't know the answer, say that you don't know.`\n",
    "\n",
    "Here's an example of giving explicit instructions to give more specific results by limiting the responses to recently created sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "Explain the latest advances in large language models to me.\n",
      "==============\n",
      "Large language models have made tremendous progress in recent years, and I'm excited to share some of the latest advancements with you!\n",
      "\n",
      "**1. Transformer Architecture:**\n",
      "The transformer architecture, introduced by Vaswani et al. in 2017, has been instrumental in achieving state-of-the-art results in natural language processing (NLP) tasks. The transformer's self-attention mechanism allows it to model complex relationships between input elements more effectively than traditional recurrent neural networks (RNNs).\n",
      "\n",
      "**2. Scaling Up:**\n",
      "The development of larger and more powerful models has been a key driver of progress. Models like BERT, RoBERTa, and XLNet have achieved remarkable results by scaling up the transformer architecture to hundreds of millions or even billions of parameters.\n",
      "\n",
      "**3. Pre-training:**\n",
      "Pre-training large language models on massive datasets (e.g., Wikipedia, BookCorpus) has become a popular approach. This involves training the model on a wide range of tasks, such as masked language modeling, next sentence prediction, and sentiment analysis. The pre-trained models can then be fine-tuned for specific downstream NLP tasks.\n",
      "\n",
      "**4. Multitask Learning:**\n",
      "Multitask learning has also gained popularity, where a single model is trained to perform multiple tasks simultaneously. This approach helps the model learn more generalizable representations that can be applied to various NLP tasks.\n",
      "\n",
      "**5. Few-Shot Learning:**\n",
      "Few-shot learning, which involves training models on small datasets or even individual examples, has become increasingly important for real-world applications. Models like Prompt-Tuning and Meta-Learning have shown promising results in this area.\n",
      "\n",
      "**6. Adversarial Training:**\n",
      "Adversarial training techniques, such as adversarial attacks and defensive distillation, have been developed to improve the robustness of large language models against potential threats, like data poisoning or model manipulation.\n",
      "\n",
      "**7. Interpretability and Explainability:**\n",
      "As AI models become more pervasive, there is a growing need for interpretability and explainability. Researchers are developing techniques to provide insights into how large language models make predictions, such as attention visualization and saliency maps.\n",
      "\n",
      "**8. Multimodal Language Models:**\n",
      "The integration of multimodal data (e.g., images, audio) with text has led to the development of multimodal language models. These models can process and generate content that combines multiple modalities, enabling new applications like visual question answering and image captioning.\n",
      "\n",
      "**9. Domain Adaptation:**\n",
      "Domain adaptation techniques have been developed to enable large language models to generalize better across different domains (e.g., medical texts, social media posts). This is crucial for real-world applications where data may not always be representative of the target domain.\n",
      "\n",
      "**10. Hardware Acceleration:**\n",
      "The increasing size and computational requirements of large language models have led to a focus on hardware acceleration. Researchers are exploring custom-designed chips, like TPUs and GPUs, to speed up model training and inference.\n",
      "\n",
      "These advances in large language models have far-reaching implications for various applications, including natural language processing, text generation, machine translation, and more. As the field continues to evolve, we can expect even more exciting developments that will shape the future of AI!\n",
      "\n",
      "==============\n",
      "Explain the latest advances in large language models to me. Always cite your sources. Never cite sources older than 2020.\n",
      "==============\n",
      "Large language models have made tremendous progress in recent years, with significant advancements in areas such as language understanding, generation, and translation. Here are some of the latest developments:\n",
      "\n",
      "1. **BERT-based Models**: BERT (Bidirectional Encoder Representations from Transformers) has become a cornerstone for many natural language processing (NLP) tasks [1]. Recent models like RoBERTa [2] and DistilBERT [3] have improved upon BERT's capabilities, achieving state-of-the-art results in various benchmarks.\n",
      "\n",
      "Source:\n",
      "[1] Devlin et al. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv preprint arXiv:1810.04805.\n",
      "[2] Liu et al. (2020). RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv preprint arXiv:1907.11692.\n",
      "[3] Sanh et al. (2019). DistilBERT: Distilled BERT for Natural Language Understanding. arXiv preprint arXiv:1910.01108.\n",
      "\n",
      "2. **Transformers with Attention Mechanisms**: The introduction of attention mechanisms in transformer architectures has led to significant improvements in language modeling and machine translation tasks [4]. Models like T5 [5] and UniLM [6] have demonstrated impressive results, leveraging the power of attention to generate coherent and accurate text.\n",
      "\n",
      "Source:\n",
      "[4] Vaswani et al. (2017). Attention Is All You Need. Advances in Neural Information Processing Systems 30.\n",
      "[5] Raffel et al. (2020). Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. arXiv preprint arXiv:1910.10683.\n",
      "[6] Wang et al. (2019). UniLM: A Unified Language Model Pre-training for Natural Language Processing and Generation. arXiv preprint arXiv:1907.12204.\n",
      "\n",
      "3. **Multitask Learning**: Recent advancements in multitask learning have enabled models to learn multiple tasks simultaneously, leading to improved performance and more robust language understanding [7]. Models like MuL-T5 [8] and BART [9] demonstrate the effectiveness of this approach.\n",
      "\n",
      "Source:\n",
      "[7] Liu et al. (2020). Multitask Learning for Natural Language Processing: A Survey. arXiv preprint arXiv:2004.09455.\n",
      "[8] Chen et al. (2020). MuL-T5: A Multi-Task Learning Framework for Text-to-Text Generation. arXiv preprint arXiv:2012.10216.\n",
      "[9] Lewis et al. (2020). BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Summarization. arXiv preprint arXiv:1910.13461.\n",
      "\n",
      "4. **Efficient Large-Scale Models**: With the growing demand for large-scale language models, researchers have focused on developing more efficient architectures that can handle increased model sizes while maintaining performance [10]. Models like BigBird [11] and Longformer [12] demonstrate the effectiveness of these approaches.\n",
      "\n",
      "Source:\n",
      "[10] Wang et al. (2020). Efficient Large-Scale Language Models. arXiv preprint arXiv:2012.11995.\n",
      "[11] Zhang et al. (2020). BigBird: Scalable Embeddings for Efficient Text Classification and Language Modeling. arXiv preprint arXiv:2010.05612.\n",
      "[12] Beltagy et al. (2020). Longformer: The Long-Range Arena of Transformer-Based Language Models. arXiv preprint arXiv:2004.08272.\n",
      "\n",
      "These advancements have significantly improved the performance and capabilities of large language models, enabling applications in natural language processing, machine translation, and text generation.\n",
      "\n",
      "References:\n",
      "\n",
      "[1] Devlin et al. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. arXiv preprint arXiv:1810.04805.\n",
      "[2] Liu et al. (2020). RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv preprint arXiv:1907.11692.\n",
      "[3] Sanh et al. (2019). DistilBERT: Distilled BERT for Natural Language Understanding. arXiv preprint arXiv:1910.01108.\n",
      "[4] Vaswani et al. (2017). Attention Is All You Need. Advances in Neural Information Processing Systems 30.\n",
      "[5] Raffel et al. (2020). Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. arXiv preprint arXiv:1910.10683.\n",
      "[6] Wang et al. (2019). UniLM: A Unified Language Model Pre-training for Natural Language Processing and Generation. arXiv preprint arXiv:1907.12204.\n",
      "[7] Liu et al. (2020). Multitask Learning for Natural Language Processing: A Survey. arXiv preprint arXiv:2004.09455.\n",
      "[8] Chen et al. (2020). MuL-T5: A Multi-Task Learning Framework for Text-to-Text Generation. arXiv preprint arXiv:2012.10216.\n",
      "[9] Lewis et al. (2020). BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Summarization. arXiv preprint arXiv:1910.13461.\n",
      "[10] Wang et al. (2020). Efficient Large-Scale Language Models. arXiv preprint arXiv:2012.11995.\n",
      "[11] Zhang et al. (2020). BigBird: Scalable Embeddings for Efficient Text Classification and Language Modeling. arXiv preprint arXiv:2010.05612.\n",
      "[12] Beltagy et al. (2020). Longformer: The Long-Range Arena of Transformer-Based Language Models. arXiv preprint arXiv:2004.08272.\n",
      "\n",
      "Please note that the references provided are a selection of recent and relevant papers, but not exhaustive.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complete_and_print(\"Explain the latest advances in large language models to me.\")\n",
    "# More likely to cite sources from 2017\n",
    "\n",
    "complete_and_print(\"Explain the latest advances in large language models to me. Always cite your sources. Never cite sources older than 2020.\")\n",
    "# Gives more specific advances and only cites sources from 2020"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Prompting using Zero- and Few-Shot Learning\n",
    "\n",
    "A shot is an example or demonstration of what type of prompt and response you expect from a large language model. This term originates from training computer vision models on photographs, where one shot was one example or instance that the model used to classify an image ([Fei-Fei et al. (2006)](http://vision.stanford.edu/documents/Fei-FeiFergusPerona2006.pdf)).\n",
    "\n",
    "#### Zero-Shot Prompting\n",
    "\n",
    "Large language models like Llama 3 are unique because they are capable of following instructions and producing responses without having previously seen an example of a task. Prompting without examples is called \"zero-shot prompting\".\n",
    "\n",
    "Let's try using Llama 3 as a sentiment detector. You may notice that output format varies - we can improve this with better prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "Text: This was the best movie I've ever seen! \n",
      " The sentiment of the text is: \n",
      "==============\n",
      "POSITIVE\n",
      "\n",
      "==============\n",
      "Text: The director was trying too hard. \n",
      " The sentiment of the text is: \n",
      "==============\n",
      "Negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complete_and_print(\"Text: This was the best movie I've ever seen! \\n The sentiment of the text is: \")\n",
    "# Returns positive sentiment\n",
    "\n",
    "complete_and_print(\"Text: The director was trying too hard. \\n The sentiment of the text is: \")\n",
    "# Returns negative sentiment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Few-Shot Prompting\n",
    "\n",
    "Adding specific examples of your desired output generally results in more accurate, consistent output. This technique is called \"few-shot prompting\".\n",
    "\n",
    "In this example, the generated response follows our desired format that offers a more nuanced sentiment classifer that gives a positive, neutral, and negative response confidence percentage.\n",
    "\n",
    "See also: [Zhao et al. (2021)](https://arxiv.org/abs/2102.09690), [Liu et al. (2021)](https://arxiv.org/abs/2101.06804), [Su et al. (2022)](https://arxiv.org/abs/2209.01975), [Rubin et al. (2022)](https://arxiv.org/abs/2112.08633).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: I thought it was okay\n",
      "20% positive 60% neutral 20% negative\n",
      "INPUT: I loved it!\n",
      "100% positive 0% neutral 0% negative\n",
      "INPUT: Terrible service 0/10\n",
      "0% positive 0% neutral 100% negative\n"
     ]
    }
   ],
   "source": [
    "def sentiment(text):\n",
    "    response = chat_completion(messages=[\n",
    "        user(\"You are a sentiment classifier. For each message, give the percentage of positive/netural/negative.\"),\n",
    "        user(\"I liked it\"),\n",
    "        assistant(\"70% positive 30% neutral 0% negative\"),\n",
    "        user(\"It could be better\"),\n",
    "        assistant(\"0% positive 50% neutral 50% negative\"),\n",
    "        user(\"It's fine\"),\n",
    "        assistant(\"25% positive 50% neutral 25% negative\"),\n",
    "        user(text),\n",
    "    ])\n",
    "    return response\n",
    "\n",
    "def print_sentiment(text):\n",
    "    print(f'INPUT: {text}')\n",
    "    print(sentiment(text))\n",
    "\n",
    "print_sentiment(\"I thought it was okay\")\n",
    "# More likely to return a balanced mix of positive, neutral, and negative\n",
    "print_sentiment(\"I loved it!\")\n",
    "# More likely to return 100% positive\n",
    "print_sentiment(\"Terrible service 0/10\")\n",
    "# More likely to return 100% negative"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Role Prompting\n",
    "\n",
    "Llama will often give more consistent responses when given a role ([Kong et al. (2023)](https://browse.arxiv.org/pdf/2308.07702.pdf)). Roles give context to the LLM on what type of answers are desired.\n",
    "\n",
    "Let's use Llama 3 to create a more focused, technical response for a question around the pros and cons of using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "Explain the pros and cons of using PyTorch.\n",
      "==============\n",
      "PyTorch is an open-source machine learning library developed by Facebook's AI Research Lab (FAIR). It has gained popularity in recent years due to its ease of use, flexibility, and rapid development capabilities. Here are some pros and cons of using PyTorch:\n",
      "\n",
      "Pros:\n",
      "\n",
      "1. **Dynamic computation graph**: PyTorch allows for dynamic computation graphs, which means you can modify the graph during runtime, making it easier to implement complex models.\n",
      "2. **Ease of use**: PyTorch has a simpler and more intuitive API compared to TensorFlow, making it easier to learn and use, especially for beginners.\n",
      "3. **Rapid prototyping**: PyTorch's dynamic computation graph and automatic differentiation capabilities enable rapid prototyping and experimentation with different models and architectures.\n",
      "4. **Flexibility**: PyTorch supports both imperative and declarative programming styles, allowing you to write code in a way that feels most natural to you.\n",
      "5. **Large community**: PyTorch has a growing and active community, which means there are many resources available for learning and troubleshooting.\n",
      "6. **Support for GPU acceleration**: PyTorch provides built-in support for GPU acceleration, making it well-suited for large-scale deep learning applications.\n",
      "7. **Open-source**: As an open-source library, PyTorch is free to use and distribute, and its development is driven by the community.\n",
      "\n",
      "Cons:\n",
      "\n",
      "1. **Steeper learning curve for advanced topics**: While PyTorch's API is easy to learn, some of its more advanced features and concepts (such as automatic differentiation) can be challenging to grasp.\n",
      "2. **Limited support for distributed training**: Compared to TensorFlow, PyTorch has limited support for distributed training, which can make it less suitable for very large-scale deep learning applications.\n",
      "3. **Less mature ecosystem**: While PyTorch's community is growing rapidly, its ecosystem is still less mature than TensorFlow's, with fewer pre-built models and tools available.\n",
      "4. **Limited support for legacy code**: PyTorch's API is designed to be easy to use, but this means that it may not be compatible with legacy code written in other frameworks like TensorFlow or Theano.\n",
      "5. **Debugging challenges**: Due to its dynamic computation graph, debugging can be more challenging in PyTorch than in other frameworks.\n",
      "6. **Less support for deployment**: While PyTorch provides some tools for deploying models, such as TorchScript and ONNX, it is still less mature in this area compared to TensorFlow.\n",
      "7. **Limited support for reinforcement learning**: While PyTorch has some built-in support for reinforcement learning, it is not as comprehensive as TensorFlow's RL library.\n",
      "\n",
      "In summary, PyTorch is a great choice when you need to quickly prototype and experiment with different models and architectures, or when you're working on smaller-scale deep learning projects. However, if you're working on very large-scale applications or require more advanced features for distributed training, deployment, or reinforcement learning, TensorFlow might be a better fit.\n",
      "\n",
      "==============\n",
      "Your role is a machine learning expert who gives highly technical advice to senior engineers who work with complicated datasets. Explain the pros and cons of using PyTorch.\n",
      "==============\n",
      "As a machine learning expert, I'd be happy to provide you with an in-depth analysis of the pros and cons of using PyTorch.\n",
      "\n",
      "**Pros:**\n",
      "\n",
      "1. **Dynamic computation graph**: PyTorch's dynamic computation graph allows for more flexibility during model development and training. This is particularly useful when working with complex models or experimenting with different architectures.\n",
      "2. **Easy debugging**: The ability to inspect and manipulate the computation graph at runtime makes it easier to debug issues and understand how the model is behaving.\n",
      "3. **Autograd**: PyTorch's autograd system automatically computes gradients for you, which simplifies the process of training neural networks.\n",
      "4. **Pythonic API**: PyTorch has a Pythonic API that allows for more concise and readable code compared to other deep learning frameworks like TensorFlow.\n",
      "5. **GPU support**: PyTorch has excellent GPU support, making it well-suited for large-scale deep learning tasks.\n",
      "6. **Large community**: PyTorch has a growing and active community, which translates to a wealth of resources, tutorials, and pre-built models available.\n",
      "\n",
      "**Cons:**\n",
      "\n",
      "1. **Steeper learning curve**: Due to its dynamic computation graph and autograd system, PyTorch can be more challenging to learn for beginners or those without prior experience with deep learning frameworks.\n",
      "2. **Less mature ecosystem**: While PyTorch's community is growing rapidly, it still lags behind TensorFlow in terms of the number of pre-built models and libraries available.\n",
      "3. **Limited support for distributed training**: PyTorch's support for distributed training is not as robust as some other frameworks, which can limit its scalability for very large-scale deep learning tasks.\n",
      "4. **No official support for Windows**: While it's possible to use PyTorch on Windows, there is no official support or pre-built binaries available, which can make installation and maintenance more challenging.\n",
      "5. **Limited support for deployment**: PyTorch's focus on research and development means that its support for deployment and production-ready models is not as comprehensive as some other frameworks.\n",
      "\n",
      "**When to use PyTorch:**\n",
      "\n",
      "1. **Research and prototyping**: PyTorch's flexibility and ease of debugging make it an excellent choice for researchers and developers who need to quickly experiment with new ideas or architectures.\n",
      "2. **Small-scale deep learning tasks**: For smaller-scale deep learning tasks, PyTorch's performance and ease of use can be a significant advantage.\n",
      "3. **Custom models**: When building custom models that require fine-grained control over the computation graph, PyTorch's dynamic nature can be particularly useful.\n",
      "\n",
      "**When not to use PyTorch:**\n",
      "\n",
      "1. **Large-scale deep learning tasks**: For extremely large-scale deep learning tasks that require distributed training and high-performance computing, other frameworks like TensorFlow or Horovod may be more suitable.\n",
      "2. **Production-ready models**: If you're looking for a framework with comprehensive support for deploying and serving production-ready models, PyTorch may not be the best choice.\n",
      "\n",
      "In conclusion, PyTorch is an excellent choice for researchers, developers, and engineers who value flexibility, ease of debugging, and Pythonic APIs. However, it's essential to carefully consider the pros and cons before deciding whether PyTorch is the right fit for your specific use case.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complete_and_print(\"Explain the pros and cons of using PyTorch.\")\n",
    "# More likely to explain the pros and cons of PyTorch covers general areas like documentation, the PyTorch community, and mentions a steep learning curve\n",
    "\n",
    "complete_and_print(\"Your role is a machine learning expert who gives highly technical advice to senior engineers who work with complicated datasets. Explain the pros and cons of using PyTorch.\")\n",
    "# Often results in more technical benefits and drawbacks that provide more technical details on how model layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain-of-Thought\n",
    "\n",
    "Simply adding a phrase encouraging step-by-step thinking \"significantly improves the ability of large language models to perform complex reasoning\" ([Wei et al. (2022)](https://arxiv.org/abs/2201.11903)). This technique is called \"CoT\" or \"Chain-of-Thought\" prompting.\n",
    "\n",
    "Llama 3 now reasons step-by-step naturally without the addition of the phrase. This section remains for completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "Who lived longer, Mozart or Elvis?\n",
      "==============\n",
      "Wolfgang Amadeus Mozart was born on January 27, 1756, and died on December 5, 1791, at the age of 35.\n",
      "\n",
      "Elvis Presley was born on January 8, 1935, and died on August 16, 1977, at the age of 42.\n",
      "\n",
      "So, Elvis lived longer than Mozart by about 7 years.\n",
      "\n",
      "==============\n",
      "Who lived longer, Mozart or Elvis? Let's think through this carefully, step by step.\n",
      "==============\n",
      "Let's break it down step by step!\n",
      "\n",
      "1. Wolfgang Amadeus Mozart was born on January 27, 1756.\n",
      "2. Elvis Presley was born on January 8, 1935.\n",
      "\n",
      "Now, let's compare their lifespans:\n",
      "\n",
      "Mozart died on December 5, 1791, at the age of 35.\n",
      "\n",
      "Elvis Presley died on August 16, 1977, at the age of 42.\n",
      "\n",
      "So, Elvis Presley lived longer than Mozart.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Who lived longer, Mozart or Elvis?\"\n",
    "\n",
    "complete_and_print(prompt)\n",
    "# Llama 2 would often give the incorrect answer of \"Mozart\"\n",
    "\n",
    "complete_and_print(f\"{prompt} Let's think through this carefully, step by step.\")\n",
    "# Gives the correct answer \"Elvis\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Consistency\n",
    "\n",
    "LLMs are probablistic, so even with Chain-of-Thought, a single generation might produce incorrect results. Self-Consistency ([Wang et al. (2022)](https://arxiv.org/abs/2203.11171)) introduces enhanced accuracy by selecting the most frequent answer from multiple generations (at the cost of higher compute):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answers: ['50', '50', '50', '50', '50']\n",
      " Final answer: 50\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from statistics import mode\n",
    "\n",
    "def gen_answer():\n",
    "    response = completion(\n",
    "        \"John found that the average of 15 numbers is 40.\"\n",
    "        \"If 10 is added to each number then the mean of the numbers is?\"\n",
    "        \"Report the answer surrounded by backticks (example: `123`)\",\n",
    "    )\n",
    "    match = re.search(r'`(\\d+)`', response)\n",
    "    if match is None:\n",
    "        return None\n",
    "    return match.group(1)\n",
    "\n",
    "answers = [gen_answer() for i in range(5)]\n",
    "\n",
    "print(\n",
    "    f\"Answers: {answers}\\n\",\n",
    "    f\"Final answer: {mode(answers)}\",\n",
    "    )\n",
    "\n",
    "# Sample runs of Llama-3-70B (all correct):\n",
    "# ['60', '50', '50', '50', '50'] -> 50\n",
    "# ['50', '50', '50', '60', '50'] -> 50\n",
    "# ['50', '50', '60', '50', '50'] -> 50"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval-Augmented Generation\n",
    "\n",
    "You'll probably want to use factual knowledge in your application. You can extract common facts from today's large models out-of-the-box (i.e. using just the model weights):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "What is the capital of the California?\n",
      "==============\n",
      "The capital of California is Sacramento.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complete_and_print(\"What is the capital of the California?\")\n",
    "# Gives the correct answer \"Sacramento\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, more specific facts, or private information, cannot be reliably retrieved. The model will either declare it does not know or hallucinate an incorrect answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "What was the temperature in Menlo Park on December 12th, 2023?\n",
      "==============\n",
      "I'd be happy to help you with that!\n",
      "\n",
      "According to historical weather data, the temperature in Menlo Park, California on December 12th, 2023 was around 48°F (9°C). However, please note that this information is subject to change and may not reflect the current or up-to-date temperatures. If you're looking for more accurate or real-time information, I recommend checking a reliable weather source or website!\n",
      "\n",
      "==============\n",
      "What time is my dinner reservation on Saturday and what should I wear?\n",
      "==============\n",
      "I'd be happy to help!\n",
      "\n",
      "According to our previous conversation, you have a dinner reservation at 7:00 PM on Saturday.\n",
      "\n",
      "As for what to wear, I believe we previously discussed that the restaurant has a smart casual dress code. You might want to consider wearing something elegant yet comfortable, such as a pair of dark jeans and a nice blouse or a button-down shirt. A blazer or cardigan can add a touch of sophistication if you like.\n",
      "\n",
      "Please let me know if this information is correct or if I need to update it for you!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complete_and_print(\"What was the temperature in Menlo Park on December 12th, 2023?\")\n",
    "# \"I'm just an AI, I don't have access to real-time weather data or historical weather records.\"\n",
    "\n",
    "complete_and_print(\"What time is my dinner reservation on Saturday and what should I wear?\")\n",
    "# \"I'm not able to access your personal information [..] I can provide some general guidance\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieval-Augmented Generation, or RAG, describes the practice of including information in the prompt you've retrived from an external database ([Lewis et al. (2020)](https://arxiv.org/abs/2005.11401v4)). It's an effective way to incorporate facts into your LLM application and is more affordable than fine-tuning which may be costly and negatively impact the foundational model's capabilities.\n",
    "\n",
    "This could be as simple as a lookup table or as sophisticated as a [vector database]([FAISS](https://github.com/facebookresearch/faiss)) containing all of your company's knowledge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "Given the following information: 'The temperature in Menlo Park was 51 degrees Fahrenheit on 2023-12-12'', respond to: 'What is the temperature in Menlo Park on 2023-12-12?'\n",
      "==============\n",
      "According to the given information, the temperature in Menlo Park on 2023-12-12 is 51 degrees Fahrenheit.\n",
      "\n",
      "==============\n",
      "Given the following information: 'The temperature in Menlo Park was unknown temperature on 2023-07-18'', respond to: 'What is the temperature in Menlo Park on 2023-07-18?'\n",
      "==============\n",
      "According to the given information, the temperature in Menlo Park on 2023-07-18 is unknown.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MENLO_PARK_TEMPS = {\n",
    "    \"2023-12-11\": \"52 degrees Fahrenheit\",\n",
    "    \"2023-12-12\": \"51 degrees Fahrenheit\",\n",
    "    \"2023-12-13\": \"51 degrees Fahrenheit\",\n",
    "}\n",
    "\n",
    "\n",
    "def prompt_with_rag(retrived_info, question):\n",
    "    complete_and_print(\n",
    "        f\"Given the following information: '{retrived_info}', respond to: '{question}'\"\n",
    "    )\n",
    "\n",
    "\n",
    "def ask_for_temperature(day):\n",
    "    temp_on_day = MENLO_PARK_TEMPS.get(day) or \"unknown temperature\"\n",
    "    prompt_with_rag(\n",
    "        f\"The temperature in Menlo Park was {temp_on_day} on {day}'\",  # Retrieved fact\n",
    "        f\"What is the temperature in Menlo Park on {day}?\",  # User question\n",
    "    )\n",
    "\n",
    "\n",
    "ask_for_temperature(\"2023-12-12\")\n",
    "# \"Sure! The temperature in Menlo Park on 2023-12-12 was 51 degrees Fahrenheit.\"\n",
    "\n",
    "ask_for_temperature(\"2023-07-18\")\n",
    "# \"I'm not able to provide the temperature in Menlo Park on 2023-07-18 as the information provided states that the temperature was unknown.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Program-Aided Language Models\n",
    "\n",
    "LLMs, by nature, aren't great at performing calculations. Let's try:\n",
    "\n",
    "$$\n",
    "((-5 + 93 * 4 - 0) * (4^4 + -7 + 0 * 5))\n",
    "$$\n",
    "\n",
    "(The correct answer is 91383.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "\n",
      "Calculate the answer to the following math problem:\n",
      "\n",
      "((-5 + 93 * 4 - 0) * (4^4 + -7 + 0 * 5))\n",
      "\n",
      "==============\n",
      "To evaluate this expression, I'll follow the order of operations (PEMDAS):\n",
      "\n",
      "1. Evaluate the expressions inside the parentheses:\n",
      "\t* (-5 + 93 * 4 - 0) = (-5 + 372 - 0) = 367\n",
      "\t* (4^4 + -7 + 0 * 5) = (256 + -7 + 0) = 249\n",
      "2. Multiply the two expressions:\n",
      "\t* 367 * 249 = 91,523\n",
      "\n",
      "So, the final answer is:\n",
      "\n",
      "91,523\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complete_and_print(\"\"\"\n",
    "Calculate the answer to the following math problem:\n",
    "\n",
    "((-5 + 93 * 4 - 0) * (4^4 + -7 + 0 * 5))\n",
    "\"\"\")\n",
    "# Gives incorrect answers like 92448, 92648, 95463"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Gao et al. (2022)](https://arxiv.org/abs/2211.10435) introduced the concept of \"Program-aided Language Models\" (PAL). While LLMs are bad at arithmetic, they're great for code generation. PAL leverages this fact by instructing the LLM to write code to solve calculation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "\n",
      "    # Python code to calculate: ((-5 + 93 * 4 - 0) * (4^4 + -7 + 0 * 5))\n",
      "    \n",
      "==============\n",
      "To evaluate this expression, we need to follow the order of operations (PEMDAS):\n",
      "\n",
      "1. Calculate the expressions inside parentheses:\n",
      "   - Inside the first parenthesis: 93 * 4 = 372, then add 5 and subtract 0: 372 + 5 - 0 = 377\n",
      "   - Inside the second parenthesis: 4^4 = 256, then add -7 and multiply by 5: 256 + (-7) * 5 = 256 - 35 = 221\n",
      "\n",
      "2. Multiply the two results:\n",
      "   - 377 * 221 = 83,333\n",
      "\n",
      "So the final result is:\n",
      "\n",
      "83,333\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complete_and_print(\n",
    "    \"\"\"\n",
    "    # Python code to calculate: ((-5 + 93 * 4 - 0) * (4^4 + -7 + 0 * 5))\n",
    "    \"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91383\n"
     ]
    }
   ],
   "source": [
    "# The following code was generated by Llama 3 70B:\n",
    "\n",
    "result = ((-5 + 93 * 4 - 0) * (4**4 - 7 + 0 * 5))\n",
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limiting Extraneous Tokens\n",
    "\n",
    "A common struggle with Llama 2 is getting output without extraneous tokens (ex. \"Sure! Here's more information on...\"), even if explicit instructions are given to Llama 2 to be concise and no preamble. Llama 3 can better follow instructions.\n",
    "\n",
    "Check out this improvement that combines a role, rules and restrictions, explicit instructions, and an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============\n",
      "Give me the zip code for Menlo Park in JSON format with the field 'zip_code'\n",
      "==============\n",
      "Here is the zip code for Menlo Park in JSON format:\n",
      "\n",
      "```\n",
      "{\n",
      "    \"zip_code\": \"94025\"\n",
      "}\n",
      "```\n",
      "\n",
      "==============\n",
      "\n",
      "    You are a robot that only outputs JSON.\n",
      "    You reply in JSON format with the field 'zip_code'.\n",
      "    Example question: What is the zip code of the Empire State Building? Example answer: {'zip_code': 10118}\n",
      "    Now here is my question: What is the zip code of Menlo Park?\n",
      "    \n",
      "==============\n",
      "{\"zip_code\": 94025}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complete_and_print(\n",
    "    \"Give me the zip code for Menlo Park in JSON format with the field 'zip_code'\",\n",
    ")\n",
    "# Likely returns the JSON and also \"Sure! Here's the JSON...\"\n",
    "\n",
    "complete_and_print(\n",
    "    \"\"\"\n",
    "    You are a robot that only outputs JSON.\n",
    "    You reply in JSON format with the field 'zip_code'.\n",
    "    Example question: What is the zip code of the Empire State Building? Example answer: {'zip_code': 10118}\n",
    "    Now here is my question: What is the zip code of Menlo Park?\n",
    "    \"\"\",\n",
    ")\n",
    "# \"{'zip_code': 94025}\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional References\n",
    "- [PromptingGuide.ai](https://www.promptingguide.ai/)\n",
    "- [LearnPrompting.org](https://learnprompting.org/)\n",
    "- [Lil'Log Prompt Engineering Guide](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author & Contact\n",
    "\n",
    "Edited by [Dalton Flanagan](https://www.linkedin.com/in/daltonflanagan/) (dalton@meta.com) with contributions from Mohsen Agsen, Bryce Bortree, Ricardo Juan Palma Duran, Kaolin Fire, Thomas Scialom."
   ]
  }
 ],
 "metadata": {
  "captumWidgetMessage": [],
  "dataExplorerConfig": [],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "last_base_url": "https://bento.edge.x2p.facebook.net/",
  "last_kernel_id": "161e2a7b-2d2b-4995-87f3-d1539860ecac",
  "last_msg_id": "4eab1242-d815b886ebe4f5b1966da982_543",
  "last_server_session_id": "4a7b41c5-ed66-4dcb-a376-22673aebb469",
  "operator_data": [],
  "outputWidgetContext": []
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
