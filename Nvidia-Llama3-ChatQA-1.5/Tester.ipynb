{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Friday, May 10, 2024\n",
    "\n",
    "mamba activate llama3\n",
    "\n",
    "[nvidia/Llama3-ChatQA-1.5-8B](https://huggingface.co/nvidia/Llama3-ChatQA-1.5-8B)\n",
    "\n",
    "This all runs in one pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only target the 4090 ...\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May 10 11:23:52 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.29.06              Driver Version: 545.29.06    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1050        Off | 00000000:01:00.0  On |                  N/A |\n",
      "|  0%   57C    P0              N/A /  70W |    403MiB /  2048MiB |      5%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 4090        Off | 00000000:02:00.0 Off |                  Off |\n",
      "|  0%   33C    P3              64W / 450W |     17MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      2253      G   /usr/lib/xorg/Xorg                          222MiB |\n",
      "|    0   N/A  N/A      2516      G   /usr/bin/gnome-shell                         40MiB |\n",
      "|    0   N/A  N/A      5501      G   ...erProcess --variations-seed-version       81MiB |\n",
      "|    0   N/A  N/A      6723      G   ...,262144 --variations-seed-version=1       52MiB |\n",
      "|    1   N/A  N/A      2253      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to use\n",
    "\n",
    "take the whole document as context\n",
    "\n",
    "This can be applied to the scenario where the whole document can be fitted into the model, so that there is no need to run retrieval over the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model_id = \"nvidia/Llama3-ChatQA-1.5-8B\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c3a6cb418004a649694d9900d7e3ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "# 233m 1.1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May 10 11:23:58 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.29.06              Driver Version: 545.29.06    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1050        Off | 00000000:01:00.0  On |                  N/A |\n",
      "|  0%   58C    P0              N/A /  70W |    388MiB /  2048MiB |      6%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 4090        Off | 00000000:02:00.0 Off |                  Off |\n",
      "|  0%   35C    P2              68W / 450W |  15850MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      2253      G   /usr/lib/xorg/Xorg                          222MiB |\n",
      "|    0   N/A  N/A      2516      G   /usr/bin/gnome-shell                         40MiB |\n",
      "|    0   N/A  N/A      5501      G   ...erProcess --variations-seed-version       67MiB |\n",
      "|    0   N/A  N/A      6723      G   ...,262144 --variations-seed-version=1       52MiB |\n",
      "|    1   N/A  N/A      2253      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    1   N/A  N/A     83101      C   ...b/miniforge3/envs/llama3/bin/python    15830MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"what is the percentage change of the net income from Q4 FY23 to Q4 FY24?\"}\n",
    "]\n",
    "\n",
    "document = \"\"\"NVIDIA (NASDAQ: NVDA) today reported revenue for the fourth quarter ended January 28, 2024, of $22.1 billion, up 22% from the previous quarter and up 265% from a year ago.\\nFor the quarter, GAAP earnings per diluted share was $4.93, up 33% from the previous quarter and up 765% from a year ago. Non-GAAP earnings per diluted share was $5.16, up 28% from the previous quarter and up 486% from a year ago.\\nQ4 Fiscal 2024 Summary\\nGAAP\\n| $ in millions, except earnings per share | Q4 FY24 | Q3 FY24 | Q4 FY23 | Q/Q | Y/Y |\\n| Revenue | $22,103 | $18,120 | $6,051 | Up 22% | Up 265% |\\n| Gross margin | 76.0% | 74.0% | 63.3% | Up 2.0 pts | Up 12.7 pts |\\n| Operating expenses | $3,176 | $2,983 | $2,576 | Up 6% | Up 23% |\\n| Operating income | $13,615 | $10,417 | $1,257 | Up 31% | Up 983% |\\n| Net income | $12,285 | $9,243 | $1,414 | Up 33% | Up 769% |\\n| Diluted earnings per share | $4.93 | $3.71 | $0.57 | Up 33% | Up 765% |\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_formatted_input(messages, context):\n",
    "    system = \"System: This is a chat between a user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions based on the context. The assistant should also indicate when the answer cannot be found in the context.\"\n",
    "    instruction = \"Please give a full and complete answer for the question.\"\n",
    "\n",
    "    for item in messages:\n",
    "        if item['role'] == \"user\":\n",
    "            ## only apply this instruction for the first user turn\n",
    "            item['content'] = instruction + \" \" + item['content']\n",
    "            break\n",
    "\n",
    "    conversation = '\\n\\n'.join([\"User: \" + item[\"content\"] if item[\"role\"] == \"user\" else \"Assistant: \" + item[\"content\"] for item in messages]) + \"\\n\\nAssistant:\"\n",
    "    formatted_input = system + \"\\n\\n\" + context + \"\\n\\n\" + conversation\n",
    "    \n",
    "    return formatted_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_input = get_formatted_input(messages, document)\n",
    "tokenized_prompt = tokenizer(tokenizer.bos_token + formatted_input, return_tensors=\"pt\").to(model.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "terminators = [\n",
    "    tokenizer.eos_token_id,\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(input_ids=tokenized_prompt.input_ids, attention_mask=tokenized_prompt.attention_mask, max_new_tokens=128, eos_token_id=terminators)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The percentage change of the net income from Q4 FY23 to Q4 FY24 is calculated using the formula (($12,285 million - $1,414 million) / $1,414 million * 100), resulting in a 769% increase.\n"
     ]
    }
   ],
   "source": [
    "response = outputs[0][tokenized_prompt.input_ids.shape[-1]:]\n",
    "print(tokenizer.decode(response, skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May 10 11:24:01 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.29.06              Driver Version: 545.29.06    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1050        Off | 00000000:01:00.0  On |                  N/A |\n",
      "|  0%   58C    P0              N/A /  70W |    388MiB /  2048MiB |      7%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 4090        Off | 00000000:02:00.0 Off |                  Off |\n",
      "|  0%   35C    P2             153W / 450W |  16458MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      2253      G   /usr/lib/xorg/Xorg                          222MiB |\n",
      "|    0   N/A  N/A      2516      G   /usr/bin/gnome-shell                         40MiB |\n",
      "|    0   N/A  N/A      5501      G   ...erProcess --variations-seed-version       67MiB |\n",
      "|    0   N/A  N/A      6723      G   ...,262144 --variations-seed-version=1       52MiB |\n",
      "|    1   N/A  N/A      2253      G   /usr/lib/xorg/Xorg                            4MiB |\n",
      "|    1   N/A  N/A     83101      C   ...b/miniforge3/envs/llama3/bin/python    16438MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
